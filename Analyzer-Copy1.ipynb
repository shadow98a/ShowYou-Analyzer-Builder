{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def post_find(self):\n",
    "        import pymongo\n",
    "        \n",
    "        client = pymongo.MongoClient(\n",
    "            \"mongodb+srv://showyou:showyou@showyou-aznp8.mongodb.net/test?retryWrites=true&w=majority\"\n",
    "        )\n",
    "        db = client.get_database('ShowYou')\n",
    "        collection = db.get_collection('post')\n",
    "        doc = collection.find()\n",
    "        # for result in doc :\n",
    "        #     print(result)\n",
    "        client.close()\n",
    "        return doc\n",
    "    \n",
    "    \n",
    "    def sentiment_analysis_result_insert(self,list):\n",
    "        import pymongo\n",
    "        \n",
    "        client = pymongo.MongoClient(\n",
    "            \"mongodb+srv://showyou:showyou@showyou-aznp8.mongodb.net/test?retryWrites=true&w=majority\"\n",
    "        )\n",
    "        db = client.get_database('ShowYou')\n",
    "        collection = db.get_collection('sentiment_analysis_result')\n",
    "        collection.drop() \n",
    "        collection.insert(list)\n",
    "        client.close()\n",
    "    \n",
    "    \n",
    "    def post_category_insert(self,list):\n",
    "        import pymongo\n",
    "        \n",
    "        client = pymongo.MongoClient(\n",
    "            \"mongodb+srv://showyou:showyou@showyou-aznp8.mongodb.net/test?retryWrites=true&w=majority\"\n",
    "        )\n",
    "        db = client.get_database('ShowYou')\n",
    "        collection = db.get_collection('post_category')\n",
    "        collection.drop() \n",
    "        collection.insert(list)\n",
    "        client.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "    was_initialized=False\n",
    "    \n",
    "    \n",
    "    def get_sentiment_analysis_model():\n",
    "        import keras\n",
    "        model=keras.models.load_model('./Analyzer/sentiment analysis model')\n",
    "        return(model)\n",
    "    \n",
    "    \n",
    "    def get_category_analysis_model():\n",
    "        import keras\n",
    "        model=keras.models.load_model('./Analyzer/category analysis model')\n",
    "        return(model)\n",
    "    \n",
    "    \n",
    "    def get_tokenizer_for_sentiment_analysis():\n",
    "        import pandas as pd\n",
    "        \n",
    "        train_data = pd.read_csv(\"./Analyzer/sentiment analysis training dataset.csv\")\n",
    "        \n",
    "        stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "        \n",
    "        import konlpy\n",
    "        from konlpy.tag import Okt\n",
    "        okt = Okt()\n",
    "        X_train = []\n",
    "        for sentence in train_data['title']:\n",
    "          temp_X = []\n",
    "          temp_X = okt.morphs(str(sentence), stem=True) # 토큰화\n",
    "          temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "          X_train.append(temp_X)\n",
    "        \n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        max_words = 35000\n",
    "        tokenizer = Tokenizer(num_words = max_words)\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        return(tokenizer)\n",
    "    \n",
    "    \n",
    "    def get_tokenizer_for_category_analysis():\n",
    "        import pandas as pd\n",
    "        \n",
    "        train_data = pd.read_csv(\"./Analyzer/category analysis training dataset.csv\")\n",
    "        \n",
    "        stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "        \n",
    "        import konlpy\n",
    "        from konlpy.tag import Okt\n",
    "        okt = Okt()\n",
    "        X_train = []\n",
    "        for sentence in train_data['title']:\n",
    "          temp_X = []\n",
    "          temp_X = okt.morphs(str(sentence), stem=True) # 토큰화\n",
    "          temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "          X_train.append(temp_X)\n",
    "        \n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        max_words = 35000\n",
    "        tokenizer = Tokenizer(num_words = max_words)\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        \n",
    "        return(tokenizer)\n",
    "    \n",
    "    \n",
    "    def get_keywordses(self):\n",
    "        test_data = self.post_find()\n",
    "        \n",
    "        stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']\n",
    "        \n",
    "        import konlpy\n",
    "        from konlpy.tag import Okt\n",
    "        okt = Okt()\n",
    "        X_test = []\n",
    "        for sentence in test_data:\n",
    "          temp_X = []\n",
    "          temp_X = okt.morphs(str(sentence['post']), stem=True) # 토큰화\n",
    "          temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "          X_test.append(temp_X)\n",
    "        \n",
    "        return(X_test)\n",
    "    \n",
    "    \n",
    "    def analyze_sentiments(self,tokenizer,X_test,model):\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        max_len = 20 # 전체 데이터의 길이를 20로 맞춘다\n",
    "        \n",
    "        X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        sentiments=[]\n",
    "        for index_for_posts in range(0,len(predictions)):\n",
    "            highest_prediction=0\n",
    "            lowest_prediction=1\n",
    "            for index_for_sentiments in range(0,3):\n",
    "                prediction_for_sentiment=predictions[index_for_posts][index_for_sentiments]\n",
    "                if prediction_for_sentiment>highest_prediction:\n",
    "                    highest_prediction=prediction_for_sentiment\n",
    "                    \n",
    "                    if index_for_sentiments==0:\n",
    "                        sentiment='-1'\n",
    "                    elif index_for_sentiments==1:\n",
    "                        sentiment='0'\n",
    "                    else:\n",
    "                        sentiment='1'\n",
    "                \n",
    "                elif prediction_for_sentiment<lowest_prediction:\n",
    "                    lowest_prediction=prediction_for_sentiment\n",
    "            \n",
    "            prediction_for_known_sentiment=highest_prediction-lowest_prediction\n",
    "            prediction_for_unknown_sentiment=lowest_prediction\n",
    "            if prediction_for_unknown_sentiment>prediction_for_known_sentiment:\n",
    "                    sentiment='0'\n",
    "            \n",
    "            row={}\n",
    "            row['post_id']=index_for_posts\n",
    "            row['sentiment']=sentiment\n",
    "            sentiments.append(row)\n",
    "        \n",
    "        self.sentiment_analysis_result_insert(sentiments)\n",
    "    \n",
    "    \n",
    "    def analyze_categories(self,tokenizer,X_test,model):\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        \n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        max_len = 20 # 전체 데이터의 길이를 20로 맞춘다\n",
    "        \n",
    "        X_test = pad_sequences(X_test, maxlen=max_len)\n",
    "        \n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        categories=[]\n",
    "        for index_for_posts in range(0,len(predictions)):\n",
    "            highest_prediction=0\n",
    "            lowest_prediction=1\n",
    "            for index_for_categories in range(0,6):\n",
    "                prediction_for_category=predictions[index_for_posts][index_for_categories]\n",
    "                if prediction_for_category>highest_prediction:\n",
    "                    highest_prediction=prediction_for_category\n",
    "                    \n",
    "                    if index_for_categories==0:\n",
    "                        category='IT'\n",
    "                    elif index_for_categories==1:\n",
    "                        category='건강'\n",
    "                    elif index_for_categories==2:\n",
    "                        category='경제'\n",
    "                    elif index_for_categories==3:\n",
    "                        category='뷰티'\n",
    "                    elif index_for_categories==4:\n",
    "                        category='생활'\n",
    "                    else:\n",
    "                        category='엔터테인먼트'\n",
    "                \n",
    "                elif prediction_for_category<lowest_prediction:\n",
    "                    lowest_prediction=prediction_for_category\n",
    "            \n",
    "            prediction_for_known_category=highest_prediction-lowest_prediction\n",
    "            prediction_for_unknown_category=lowest_prediction\n",
    "            if prediction_for_unknown_category>prediction_for_known_category:\n",
    "                    category='기타'\n",
    "            \n",
    "            row={}\n",
    "            row['post_id']=index_for_posts\n",
    "            row['category']=category\n",
    "            categories.append(row)\n",
    "        \n",
    "        self.post_category_insert(categories)\n",
    "    \n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    def __init__(cls):\n",
    "        if not cls.was_initialized:\n",
    "            print('Analyzer: Analyzer is under initialization.')\n",
    "            cls.sentiment_analysis_model=cls.get_sentiment_analysis_model()\n",
    "            cls.tokenizer_for_sentiment_analysis=cls.get_tokenizer_for_sentiment_analysis()\n",
    "            \n",
    "            cls.category_analysis_model=cls.get_category_analysis_model()\n",
    "            cls.tokenizer_for_category_analysis=cls.get_tokenizer_for_category_analysis()\n",
    "            \n",
    "            cls.was_initialized=True\n",
    "            print('Analyzer: Analyzer initialized.')\n",
    "        else:\n",
    "            print('Analyzer: Analyzer has initialized.')\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    def analyze(self):\n",
    "        print('Analyzer: Analyzer is under analysis.')\n",
    "        keywordses=self.get_keywordses()\n",
    "        self.analyze_sentiments(self.tokenizer_for_sentiment_analysis, keywordses, self.sentiment_analysis_model)\n",
    "        self.analyze_categories(self.tokenizer_for_category_analysis, keywordses, self.category_analysis_model)\n",
    "        print('Analyzer: Analyzer analyzed posts.')\n",
    "        print()\n",
    "\n",
    "\n",
    "\n",
    "def analyze():\n",
    "    analyzer=Analyzer()\n",
    "    analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: Analyzer is under initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\shadow98a\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: Analyzer initialized.\n",
      "\n",
      "Analyzer: Analyzer is under analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shadow98a\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "C:\\Users\\shadow98a\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: Analyzer analyzed posts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: Analyzer has initialized.\n",
      "\n",
      "Analyzer: Analyzer is under analysis.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shadow98a\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n",
      "C:\\Users\\shadow98a\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: DeprecationWarning: insert is deprecated. Use insert_one or insert_many instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzer: Analyzer analyzed posts.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
