{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"./Analyzer/category analysis training dataset.csv\")\n",
    "test_data = pd.read_csv(\"./Analyzer/category analysis test dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 모델을 만들기 위한 데이터 전처리 작업\n",
    "먼저 각각의 제목을 토큰화 해주었습니다.\n",
    "\n",
    "Okt형태소 분석기를 활용하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['의', '가', '이', '은', '들', '는', '좀', '잘', '걍', '과', '도', '를', '으로', '자', '에', '와', '한', '하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "for sentence in train_data['title']:\n",
    "  temp_X = []\n",
    "  temp_X = okt.morphs(str(sentence), stem=True) # 토큰화\n",
    "  temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "  X_train.append(temp_X)\n",
    "\n",
    "X_test = []\n",
    "for sentence in test_data['title']:\n",
    "  temp_X = []\n",
    "  temp_X = okt.morphs(str(sentence), stem=True) # 토큰화\n",
    "  temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "  X_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화가 잘 되었는지 출력해보면 다음과 같습니다.\n",
    "\n",
    "토큰화 한 단어를 컴퓨터가 인식할 수 있도록 정수인코딩을 해주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words = 35000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Tokenizer for category analysis','wb')\n",
    "import pickle\n",
    "pickle.dump(tokenizer,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값으로 들어갈 label -1, 0, 1을 컴퓨터가 보고 알수 있도록 one-hot encoding을 해주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(train_data['label'])):\n",
    "  if train_data['label'].iloc[i] == 'IT':\n",
    "    y_train.append([1, 0, 0, 0, 0, 0])\n",
    "  elif train_data['label'].iloc[i] == '건강':\n",
    "    y_train.append([0, 1, 0, 0, 0, 0])\n",
    "  elif train_data['label'].iloc[i] == '경제':\n",
    "    y_train.append([0, 0, 1, 0, 0, 0])\n",
    "  elif train_data['label'].iloc[i] == '뷰티':\n",
    "    y_train.append([0, 0, 0, 1, 0, 0])\n",
    "  elif train_data['label'].iloc[i] == '생활':\n",
    "    y_train.append([0, 0, 0, 0, 1, 0])\n",
    "  elif train_data['label'].iloc[i] == '엔터테인먼트':\n",
    "    y_train.append([0, 0, 0, 0, 0, 1])\n",
    "  \n",
    "\n",
    "for i in range(len(test_data['label'])):\n",
    "  if test_data['label'].iloc[i] == 'IT':\n",
    "    y_test.append([1, 0, 0, 0, 0, 0])\n",
    "  elif test_data['label'].iloc[i] == '건강':\n",
    "    y_test.append([0, 1, 0, 0, 0, 0])\n",
    "  elif test_data['label'].iloc[i] == '경제':\n",
    "    y_test.append([0, 0, 1, 0, 0, 0])\n",
    "  elif test_data['label'].iloc[i] == '뷰티':\n",
    "    y_test.append([0, 0, 0, 1, 0, 0])\n",
    "  elif test_data['label'].iloc[i] == '생활':\n",
    "    y_test.append([0, 0, 0, 0, 1, 0])\n",
    "  elif test_data['label'].iloc[i] == '엔터테인먼트':\n",
    "    y_test.append([0, 0, 0, 0, 0, 1])\n",
    "      \n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_len = 20 # 전체 데이터의 길이를 20로 맞춘다\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 필요한 것들을 import 해주고 pad_sequences를 활용하여 모든 데이터의 길이를 20으로 통일하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 100))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=1, batch_size=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "긍정, 부정, 중립 3가지로 분류해야하니 LSTM, softmax, categorical_crossentropy를 사용하였습니다.\n",
    "\n",
    "batch_size는 10 6,000개의 훈련데이터 중 10퍼센트인 600개는 validation_data로 활용하기위해 validation_split을 0.1을 부여하였습니다.\n",
    "\n",
    "optimizer는 rmsprop을 사용하여 위와 같이 모델을 만들고 학습을 시켜보았습니다.\n",
    "\n",
    "맘스터치관련 기사 제목 1,000개로 구성되어있는 테스트 데이터셋으로 평가해보니 94.27퍼센트가 나왔습니다.\n",
    "\n",
    "생각보다 너무 잘나와서 조금 이상하지만 optimizer만 adam으로 바꿔 한번 더 해보았습니다.\n",
    "\n",
    "이번엔 96.07%라는 결과가 나왔습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('category analysis model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predict_labels = np.argmax(predict, axis=1)\n",
    "original_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for i in range(len(X_test)):\n",
    "    if original_labels[i] == 0:\n",
    "        original = \"IT\"\n",
    "    elif original_labels[i] == 1:\n",
    "        original = \"건강\"\n",
    "    elif original_labels[i] == 2:\n",
    "        original = \"경제\"\n",
    "    elif original_labels[i] == 3:\n",
    "        original = \"뷰티\"\n",
    "    elif original_labels[i] == 4:\n",
    "        original = \"생활\"\n",
    "    elif original_labels[i] == 5:\n",
    "        original = \"엔터테인먼트\"\n",
    "    \n",
    "    if predict_labels[i] == 0:\n",
    "        prediction = \"IT\"\n",
    "    elif predict_labels[i] == 1:\n",
    "        prediction = \"건강\"\n",
    "    elif predict_labels[i] == 2:\n",
    "        prediction = \"경제\"\n",
    "    elif predict_labels[i] == 3:\n",
    "        prediction = \"뷰티\"\n",
    "    elif predict_labels[i] == 4:\n",
    "        prediction = \"생활\"\n",
    "    elif predict_labels[i] == 5:\n",
    "        prediction = \"엔터테인먼트\"\n",
    "        \n",
    "    print(\"기사제목 : \", test_data['title'].iloc[i], \"/\\t 원래 라벨 : \", original, \"/\\t예측한 라벨 : \", prediction)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
